import logging
import os
import random
import re
import sys
from configparser import ConfigParser
from datetime import datetime, timezone

import pymssql
import requests

from kafka import KafkaConsumer, TopicPartition

# é…ç½®å‚æ•°
KAFKA_CONNECT_SERVICE_URL = os.getenv("KAFKA_CONNECT_SERVICE_URL")
KAFKA_CONNECT_BOOTSTRAP_SERVERS = os.getenv("KAFKA_CONNECT_BOOTSTRAP_SERVERS")

if KAFKA_CONNECT_SERVICE_URL is None or KAFKA_CONNECT_BOOTSTRAP_SERVERS is None:
    print("KAFKA_CONNECT_SERVICE_URL/KAFKA_CONNECT_BOOTSTRAP_SERVERS environment variable is not set.")
    sys.exit(1)  # 1 signifies an error, 0 would indicate success

# é»˜è®¤ properties æ–‡ä»¶è·¯å¾„
DEFAULT_PROPERTIES_FILE = "connect-credentials.properties"

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# æ•°æ®åº“è¿æ¥æ± 
connections = {}

def get_db_connection(db):
    """è·å–æˆ–åˆ›å»ºæ•°æ®åº“è¿æ¥"""
    db_key = (db["hostname"], db["port"], db["database"])

    if db_key not in connections:
        try:
            connections[db_key] = pymssql.connect(
                server=db["hostname"],
                port=db["port"],
                database=db["database"],
                user="dbadmin", # db["user"],
                password="y=cos(x)+2", # db["password"],
                as_dict=True,
                login_timeout=5  # Connection timeout set to 5 seconds
            )
        except Exception as e:
            logging.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise e
        
    return connections[db_key]

# è¯»å– properties æ–‡ä»¶
def load_properties(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            config = ConfigParser()
            # ä¿æŒé”®åçš„åŸå§‹å¤§å°å†™
            config.optionxform = str  # é»˜è®¤ä¼šè½¬æ¢ä¸ºå°å†™ï¼Œæ”¹ä¸ºä¸è½¬æ¢

            properties = "[DEFAULT]\n" + f.read()
            config.read_string(properties)
            
            return dict(config["DEFAULT"])
    except Exception as e:
        print(f"è¯»å– properties æ–‡ä»¶å¤±è´¥: {e}")
        return {}

# è§£æå ä½ç¬¦ï¼Œæå–æ–‡ä»¶è·¯å¾„å’Œå˜é‡å
def extract_file_and_key(value):
    # å¦‚æœ database.hostname å½¢å¦‚ ${file:/etc/kafka-connect/secrets/connect-credentials.properties:IMX_CTO_HOSTNAME}
    # è¯·å°†å…¶æ‹†è§£ä¸º /etc/kafka-connect/secrets/connect-credentials.properties ä»¥åŠ IMX_CTO_HOSTNAME ä¸¤æ®µ

    match = re.match(r"\${file:(.*?):(.*?)}", value)
    return match.groups() if match else (None, None)

# è§£æå ä½ç¬¦å¹¶æ›¿æ¢
def resolve_placeholders(value):

    file_path, key = extract_file_and_key(value)

    # æ–‡ä»¶ä¸å­˜åœ¨æ—¶ä½¿ç”¨é»˜è®¤é…ç½®æ–‡ä»¶
    if file_path and not os.path.exists(file_path):
        file_path = DEFAULT_PROPERTIES_FILE

    if file_path and key:
        properties = load_properties(file_path)
        return properties.get(key, f"<MISSING: {key}>")  # é»˜è®¤å€¼å¤„ç†

    return value  # ä¸åŒ¹é…åˆ™è¿”å›åŸå€¼

def get_debezium_sqlserver_connectors():
    """è·å– Kafka Connect ä¸Šçš„ Debezium SQL Server è¿æ¥å™¨ä¿¡æ¯"""
    response = requests.get(f"{KAFKA_CONNECT_SERVICE_URL}/connectors?expand=info")
    if response.status_code != 200:
        logging.error(
            "è·å– Kafka Connect è¿æ¥å™¨å¤±è´¥: çŠ¶æ€ç =%d, å“åº”å†…å®¹=%s, è¯·æ±‚URL=%s",
            response.status_code,
            response.text,
            response.url
        )
        return []
    
    connectors = response.json()
    db_dict = {}
    
    # è¿‡æ»¤ Debezium SQL Server è¿æ¥å™¨
    for connector, config in connectors.items():
        config = config.get("info").get("config")
        
        # åˆ¤æ–­æ˜¯å¦ä¸º Debezium SQL Server è¿æ¥å™¨
        if config.get("connector.class") == "io.debezium.connector.sqlserver.SqlServerConnector":
            
            config = {k: resolve_placeholders(v) for k, v in config.items()}
            # fix: å…¼å®¹ Debezium 2.x ç‰ˆæœ¬çš„é…ç½®
            dbname = config.get("database.dbname") if config.get("database.dbname") else config.get("database.names")

            db_key = (dbname, config.get("database.hostname"), config.get("database.port", 1433))
            if db_key not in db_dict:
                db_dict[db_key] = {
                    "database": dbname,
                    "hostname": config.get("database.hostname"),
                    "port": config.get("database.port", 1433),
                    "user": config.get("database.user"),
                    "password": config.get("database.password"),
                    "tables": set(),
                    "active_topics": set()
                }
            # åˆå¹¶è¡¨åˆ—è¡¨
            db_dict[db_key]["tables"].update(config.get("table.include.list", "").split(","))

            active_topics = get_active_topics_of_kafka_connect(config)
            db_dict[db_key]["active_topics"].update(active_topics)

    return list(db_dict.values())

def get_active_topics_of_kafka_connect(connector_config):
    """
    è·å–æŒ‡å®š Connector å…³è”çš„ Kafka Topic
    """
    connector_name = connector_config.get("name")

    response = requests.get(f"{KAFKA_CONNECT_SERVICE_URL}/connectors/{connector_name}/topics")
    if response.status_code != 200:
        logging.error(
            "è·å– Kafka Connect è¿æ¥å™¨å¤±è´¥: çŠ¶æ€ç =%d, å“åº”å†…å®¹=%s, è¯·æ±‚URL=%s",
            response.status_code,
            response.text,
            response.url
        )
        return []
        
    connector_topics = response.json()
    return [
        topic for topic in connector_topics.get(connector_name, {}).get('topics', [])
        if '.dbo.' in topic
    ]

def check_sqlserver_cdc(db):
    """æ£€æŸ¥ SQL Server æ˜¯å¦å¯ç”¨ CDCï¼Œå¹¶æ¯”å¯¹è¡¨"""
    try:
        conn = get_db_connection(db)
        cursor = conn.cursor()

        cursor.execute("SELECT is_cdc_enabled FROM sys.databases WHERE name = %s", (db["database"]))
        is_cdc_enabled = cursor.fetchone()
        if not is_cdc_enabled:
            logging.error(f"æ•°æ®åº“ {db['database']} æœªå¯ç”¨ CDCï¼")
            return False
        
        cursor.execute("SELECT name FROM sys.tables WHERE is_tracked_by_cdc = 1")
        cdc_tables = {"dbo." + row['name'] for row in cursor.fetchall()}
        configured_tables = set(db["tables"])
        missing_tables = configured_tables - cdc_tables
        extra_cdc_tables = cdc_tables - configured_tables
        if missing_tables:
            logging.warning(f"ä»¥ä¸‹è¡¨æœªå¯ç”¨ CDC: {', '.join(missing_tables)}")
        if extra_cdc_tables:
            logging.warning(f"ä»¥ä¸‹è¡¨å¯ç”¨äº† CDC ä½†æœªé…ç½®åŒæ­¥: {', '.join(extra_cdc_tables)}")
        return True
    except Exception as e:
        logging.error(f"æ£€æŸ¥ SQL Server CDC å¤±è´¥: {e}")
        return False

def check_sqlserver_cdc_agent_job(db):
    """æ£€æŸ¥ SQL Server CDC Agent Job æ˜¯å¦è¿è¡Œ"""
    conn = get_db_connection(db)
    cursor = conn.cursor()

    try:
        cursor.execute("EXEC msdb.dbo.sp_help_jobactivity @job_name = %s", (f'cdc.{db["database"]}_capture',))
        job_status = cursor.fetchone()
        if job_status and job_status['start_execution_date'] is not None and job_status['stop_execution_date'] is None:
            logging.info(f"CDC Agent Job {db['database']}_capture æ­£å¸¸è¿è¡Œ")
            return True

        logging.error(f"CDC or LogReader Agent Job for {db['database']} æœªè¿è¡Œï¼Œå°è¯•é‡å¯")
        return False
    except Exception as e:
        # The specified @job_name ('cdc.FISAMS_capture') does not exist.DB-Lib error message 20018, severity 16:
        # General SQL Server error: Check messages from the SQL Server

        # The capture job cannot be used by Change Data Capture to extract changes from the log when transactional replication is also enabled on the same database. 
        # When Change Data Capture and transactional replication are both enabled on a database, use the logreader agent to extract the log changes.

        cursor.execute("SELECT DATABASEPROPERTYEX(%s, 'IsPublished') AS IsPublished", (db["database"]))
        repl_status = cursor.fetchone()
        if repl_status and repl_status['IsPublished'] == 1:
            logging.info(f"{db['database']} å¯ç”¨äº† Replicationï¼Œè·³è¿‡æ£€æŸ¥ CDC Agent Job")
            logging.warning(f"{db['database']} å¯ç”¨äº† Replicationï¼Œä½†ä¸»è¦æ³¨æ„ LogReader Agent æ˜¯å®šæ—¶è¿è¡Œï¼Œæ—¶å»¶å’ŒCDCæœ‰å·®å¼‚")
            return True

        logging.error(f"CDC or LogReader Agent Job for {db['database']} å‡æœªè¿è¡Œ: {e}")
        return False

def restart_sqlserver_cdc_agent_job(db):
    """é‡å¯ CDC Agent Job"""
    try:
        # conn = get_db_connection(db)
        # cursor = conn.cursor()

        # cursor.execute("EXEC msdb.dbo.sp_start_job @job_name = %s", (f'cdc.{db["database"]}_capture'))
        logging.info(f"é‡å¯ CDC Agent Job {db['database']}_capture æˆåŠŸ")
    except Exception as e:
        logging.error(f"é‡å¯ CDC Agent Job {db['database']}_capture å¤±è´¥: {e}")

def is_sqlserver_alwayson(db):
    """æ£€æŸ¥æ˜¯å¦ä¸º AlwaysOn é›†ç¾¤"""
    try:
        conn = get_db_connection(db)
        cursor = conn.cursor()

        cursor.execute("SELECT COUNT(1) AS is_alwayson FROM sys.availability_groups")
        result = cursor.fetchone()
        return result["is_alwayson"] > 0
    except Exception as e:
        logging.error(f"æ£€æŸ¥ AlwaysOn é›†ç¾¤çŠ¶æ€å¤±è´¥: {e}")
        return False

def is_sqlserver_alwayson_primary(db):
    """æ£€æŸ¥å½“å‰å®ä¾‹æ˜¯å¦ä¸º AlwaysOn é›†ç¾¤çš„ä¸»èŠ‚ç‚¹"""
    try:
        conn = get_db_connection(db)
        cursor = conn.cursor(as_dict=True)

        # ä½¿ç”¨ sys.fn_hadr_is_primary_replica åˆ¤æ–­å½“å‰å®ä¾‹æ˜¯å¦ä¸ºä¸»èŠ‚ç‚¹
        # https://learn.microsoft.com/en-us/sql/relational-databases/system-functions/sys-fn-hadr-is-primary-replica-transact-sql?view=sql-server-ver16
        cursor.execute("""
            SELECT sys.fn_hadr_is_primary_replica(%s) AS is_primary
        """, (db["database"],))
        result = cursor.fetchone()

        if result and (result['is_primary'] == 1 or result['is_primary'] is None):
            logging.info(f"å½“å‰æ•°æ®åº“ {db['database']} æ‰€åœ¨å®ä¾‹ {db['hostname']} æ˜¯ä¸»èŠ‚ç‚¹")
            return True
        else: # is_primary ä¸º 0 åˆ™ä¸æ˜¯ä¸»èŠ‚ç‚¹
            logging.info(f"å½“å‰æ•°æ®åº“ {db['database']} æ‰€åœ¨å®ä¾‹ {db['hostname']} ä¸æ˜¯ä¸»èŠ‚ç‚¹")
            return False
    except Exception as e:
        logging.error(f"æ£€æŸ¥ AlwaysOn ä¸»èŠ‚ç‚¹çŠ¶æ€å¤±è´¥: {e}")
        return False

def get_sqlserver_alwayson_primary(db):
    """è·å– AlwaysOn é›†ç¾¤çš„ä¸»èŠ‚ç‚¹åœ°å€å¹¶æ›´æ–°æ•°æ®åº“é…ç½®"""
    try:
        conn = get_db_connection(db)
        cursor = conn.cursor()
        
        # 
        cursor.execute("""
            SELECT primary_replica 
            FROM sys.dm_hadr_availability_group_states;
        """)
        result = cursor.fetchone()
        if result:
            logging.info(f"ä¸»èŠ‚ç‚¹åœ°å€: {result['primary_replica']}")
            # ä¿®æ”¹æ•°æ®åº“è¿æ¥ä¿¡æ¯æŒ‡å‘ä¸»èŠ‚ç‚¹
            db["hostname"] = result["primary_replica"]
        return db
    except Exception as e:
        logging.error(f"è·å– AlwaysOn ä¸»èŠ‚ç‚¹åœ°å€å¤±è´¥: {e}")
        return db

def check_sqlserver_alwayson_status(db):
    """æ£€æŸ¥ SQL Server AlwaysOn çŠ¶æ€"""
    try:
        conn = get_db_connection(db)
        cursor = conn.cursor(as_dict=True)

        cursor.execute("""
            SELECT synchronization_state_desc
            FROM sys.dm_hadr_database_replica_states
            WHERE database_id = DB_ID(%s) 
                AND is_primary_replica = 0
        """, (db["database"],))
        status = cursor.fetchone()
        if status and status['synchronization_state_desc'] != "SYNCHRONIZING":
            logging.warning(f"æ•°æ®åº“ {db['database']} åœ¨ AlwaysOn ç»„ä¸­åŒæ­¥çŠ¶æ€å¼‚å¸¸: {status['synchronization_state_desc']}")
        else:
            logging.info(f"æ•°æ®åº“ {db['database']} åŒæ­¥çŠ¶æ€æ­£å¸¸")
        return status['synchronization_state_desc'] if status else "UNKNOWN"
    except Exception as e:
        logging.error(f"æ£€æŸ¥ AlwaysOn çŠ¶æ€å¤±è´¥: {e}")
        return "UNKNOWN"

def check_and_repair_sqlserver_datasource(dbs):

    logging.info("å¼€å§‹ CDC æ•°æ®åŒæ­¥å·¡æ£€")
    logging.debug(f"æ•°æ®åº“åˆ—è¡¨: {dbs}")
    
    for db in dbs:
        logging.info("-------------------------------------------------")
        logging.info(f"æ£€æŸ¥æ•°æ®åº“: {db['database']} ({db['hostname']})")
        if check_sqlserver_cdc(db) is False:
            # TODO:æ¢å¤CDC
            continue

        if is_sqlserver_alwayson(db):
            # æ£€æŸ¥ AlwaysOn ä¸»ä»åŒæ­¥çŠ¶æ€
            if check_sqlserver_alwayson_status(db) is False:
                # TODO:æ¢å¤ä¸»ä»åŒæ­¥çŠ¶æ€
                continue

            # å¦‚æœå½“å‰èŠ‚ç‚¹æ˜¯ SQL Server AlwaysOn é›†ç¾¤çš„è¾…åŠ©è§’è‰²ï¼Œéœ€åˆ‡æ¢åˆ°ä¸»è¦è§’è‰²ï¼Œ
            # å¦åˆ™æ— æ³•æŸ¥è¯¢ Agent è¿è¡ŒçŠ¶æ€
            if not is_sqlserver_alwayson_primary(db):
                db = get_sqlserver_alwayson_primary(db)
        
        # æ£€æŸ¥ SQL Server CDC Agent Job æ˜¯å¦è¿è¡Œ
        if check_sqlserver_cdc_agent_job(db) is False:
            # TODO:æ¢å¤CDC Agent Job
            restart_sqlserver_cdc_agent_job(db)
            continue
    
    logging.info("CDC æ•°æ®åŒæ­¥å·¡æ£€å®Œæˆ")

def check_and_restart_failed_kafka_connectors():
    # è·å–æ‰€æœ‰ Connector çŠ¶æ€ä¿¡æ¯
    response = requests.get(f"{KAFKA_CONNECT_SERVICE_URL}/connectors?expand=status")
    
    if response.status_code != 200:
        logging.error(
            "è·å– Kafka Connect è¿æ¥å™¨å¤±è´¥: çŠ¶æ€ç =%d, å“åº”å†…å®¹=%s, è¯·æ±‚URL=%s",
            response.status_code,
            response.text,
            response.url
        )
        return False
    
    connectors_status = response.json()
    
    failed_tasks = []
    for connector_name, details in connectors_status.items():
        tasks = details.get("status").get("tasks")
        for task in tasks:
            if task.get("state") == "FAILED":
                failed_tasks.append((connector_name, task.get("id")))
    
    # é‡å¯å¤±è´¥çš„ä»»åŠ¡
    if not failed_tasks:
        logging.info("No failed connectors found.")
        return True
    
    for connector_name, task_id in failed_tasks:
        restart_response = requests.post(f"{KAFKA_CONNECT_SERVICE_URL}/connectors/{connector_name}/tasks/{task_id}/restart")
        
        if restart_response.status_code == 200:
            logging.info(f"Successfully restarted task {task_id} of connector {connector_name}.")
        else:
            logging.info(f"Failed to restart task {task_id} of connector {connector_name}.")
    
    return True

def extract_table_name_from_topic(topic):
    """
    ä» Kafka Topic ä¸­æå–è¡¨å
    ä¾‹å¦‚ï¼š
    - input: "dbserver.dbo.users" -> output: "users"
    - input: "dbserver.dbo.orders" -> output: "orders"
    """
    parts = topic.split(".")
    if len(parts) > 2:
        return parts[-1]
    return topic

def get_sqlserver_ct_time(db, table_name):
    """
    æŸ¥è¯¢ SQL Server CT è¡¨çš„æœ€æ–°è®°å½•æ—¶é—´ï¼Œå¹¶åŠ¨æ€è½¬æ¢ä¸º UTC Unix æ—¶é—´æˆ³ (æ¯«ç§’çº§)
    """
    try:
        conn = get_db_connection(db)  # è·å–æ•°æ®åº“è¿æ¥
        cursor = conn.cursor()

        # è·å– SQL Server å½“å‰æ—¶åŒºåç§»é‡
        cursor.execute("SELECT SYSDATETIMEOFFSET() as server_local_time")
        server_local_time = cursor.fetchone()['server_local_time']  # è¿”å›å¸¦æ—¶åŒºçš„ datetime å¯¹è±¡
        
        # è§£æ SQL Server çš„æ—¶åŒºåç§»é‡
        utcoffset = server_local_time.utcoffset()  # è·å–æ—¶åŒºåç§»é‡ (timedelta)

        # æŸ¥è¯¢æœ€æ–°çš„ CDC è®°å½•æ—¶é—´
        cursor.execute(f"""
            SELECT TOP 1 
                sys.fn_cdc_map_lsn_to_time(__$start_lsn) AS latest_lsn_time,
                *
            FROM [cdc].[dbo_{table_name}_CT]
            ORDER BY __$start_lsn DESC;
        """)

        latest_record = cursor.fetchone()
        
        if latest_record:
            latest_lsn_time = latest_record['latest_lsn_time']  # SQL Server è¿”å›çš„æœ¬åœ°æ—¶é—´ (without timezone)
            
            # æ—¶é—´é™„åŠ ä¸Šæ—¶åŒºä¿¡æ¯ 
            latest_lsn_time = latest_lsn_time.replace(tzinfo=timezone(utcoffset))  
            
            # è½¬æ¢ä¸º UTC
            lsn_utc_time = latest_lsn_time.astimezone(timezone.utc)
            
            # è½¬æ¢ä¸º Unix æ—¶é—´æˆ³ (æ¯«ç§’çº§)
            return int(lsn_utc_time.timestamp() * 1000)  

        return None
    except Exception as e:
        logging.error(f"æ£€æŸ¥ SQL Server CT å¤±è´¥: {e}")
        return None

def get_kafka_ct_time(topic_name):
    """
    è·å– Kafka Topic å„åˆ†åŒºæœ€æ–°çš„ä¸€æ¡æ¶ˆæ¯ï¼Œå¹¶è¿”å›æœ€æ–°çš„æ—¶é—´æˆ³
    """
    consumer = KafkaConsumer(
        bootstrap_servers=KAFKA_CONNECT_BOOTSTRAP_SERVERS,
        enable_auto_commit=False
    )

    # è·å– topic çš„æ‰€æœ‰åˆ†åŒº
    partitions = consumer.partitions_for_topic(topic_name)
    if not partitions:
        print(f"No partitions found for topic: {topic_name}")
        return None

    # æŒ‡å®šæ¶ˆè´¹çš„åˆ†åŒº
    tp_list = [TopicPartition(topic_name, p) for p in partitions]
    consumer.assign(tp_list)

    # è·å–å„åˆ†åŒºçš„æœ€æ–° offset
    end_offsets = consumer.end_offsets(tp_list)
    latest_timestamp = None

    for tp, offset in end_offsets.items():
        if offset == 0:
            continue  # è¯¥åˆ†åŒºæ²¡æœ‰æ•°æ®ï¼Œè·³è¿‡
        consumer.seek(tp, offset - 1)  # å®šä½åˆ°æœ€åä¸€æ¡æ¶ˆæ¯
        for msg in consumer:
            if latest_timestamp is None or msg.timestamp > latest_timestamp:
                latest_timestamp = msg.timestamp
            break  # åªå–ä¸€æ¡

    consumer.close()

    # long int æ˜¯å¦è¦è½¬æ¢æˆ timestamp ?
    return latest_timestamp if latest_timestamp else None

def check_sqlserver2kafka_sync(ds):
    for db in ds:
        logging.info("-------------------------------------------------")
        logging.info(f"æ£€æŸ¥æ•°æ®åº“: {db['database']} ({db['hostname']})")

        # å¤„ç† active_topicsï¼Œéšæœºé€‰æ‹©ä¸€ä¸ª Topic
        active_topics = db.get("active_topics", [])
        if not active_topics:
            logging.warning(f"[WARN] æ•°æ®åº“ {db['database']} æ²¡æœ‰ active_topics")
            continue

        random_topic = random.choice(list(active_topics))
        table_name = extract_table_name_from_topic(random_topic)

        logging.info(f"ğŸ² éšæœºé€‰æ‹© Topic: {random_topic} -> è¡¨å: {table_name}")

         # æŸ¥è¯¢ CT è¡¨æœ€æ–°æ—¶é—´
        ct_time = get_sqlserver_ct_time(db, table_name)
        if not ct_time:
            logging.warning(f"[WARN] æŸ¥è¯¢ CT è¡¨ {table_name} æ²¡æœ‰æ•°æ®")
            continue

        # æŸ¥è¯¢ Kafka æœ€æ–°æ¶ˆè´¹æ—¶é—´
        kafka_time = get_kafka_ct_time(random_topic)
        if not kafka_time:
            logging.warning(f"[WARN] æŸ¥è¯¢ Topic {random_topic} æ²¡æœ‰æ•°æ®")
            continue

        # è®¡ç®—æ—¶é—´å·®
        time_diff = (kafka_time - ct_time) / 1000
        logging.info(f"ğŸ“Š æ—¶é—´å·®: {time_diff:.3f} ç§’")

        if time_diff > 60:
            logging.warning(f"[WARN] æ—¶é—´å·®è¶…è¿‡é˜ˆå€¼: {time_diff:.3f} ç§’")
            # TODO:å°è¯•é‡å¯ Debezium Connect

        else:
            logging.info(f"âœ… æ—¶é—´å·®åœ¨é˜ˆå€¼èŒƒå›´å†…: {time_diff:.3f} ç§’")

    return True

# æ£€æµ‹ CDC Topic æ˜¯å¦æœ‰ Lag
def check_kafka2postgres_sync():
    return True


def main():
    try:
        ds = get_debezium_sqlserver_connectors()
        
        check_and_repair_sqlserver_datasource(ds)

        check_sqlserver2kafka_sync(ds)

        check_kafka2postgres_sync()

    except Exception as e:
        logging.error(f"è¿è¡Œå‡ºé”™: {e}")

if __name__ == "__main__":
    main()