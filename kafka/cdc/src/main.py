import json
import logging
import os
import random
import re
import sys
from configparser import ConfigParser
from datetime import datetime, timezone

import psycopg2
import pymssql
import requests
from confluent_kafka import (Consumer, ConsumerGroupTopicPartitions,
                             KafkaException, TopicPartition)
from confluent_kafka.admin import AdminClient

# é…ç½®å‚æ•°
KAFKA_CONNECT_SERVICE_URL = os.getenv("KAFKA_CONNECT_SERVICE_URL")
KAFKA_CONNECT_BOOTSTRAP_SERVERS = os.getenv("KAFKA_CONNECT_BOOTSTRAP_SERVERS", "")
REDPANDA_API = os.getenv("REDPANDA_API")

if KAFKA_CONNECT_SERVICE_URL is None or REDPANDA_API is None:
    print("KAFKA_CONNECT_SERVICE_URL/REDPANDA_API environment variable is not set.")
    sys.exit(1)  # 1 signifies an error, 0 would indicate success

# é»˜è®¤ properties æ–‡ä»¶è·¯å¾„
DEFAULT_PROPERTIES_FILE = "connect-credentials.properties"

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# æ•°æ®åº“è¿æ¥æ± 
connections = {}

def get_sqlserver_connection(db):
    """è·å–æˆ–åˆ›å»ºæ•°æ®åº“è¿æ¥ï¼ˆä¼˜å…ˆä½¿ç”¨ä¿ç•™è´¦å·ï¼‰"""
    db_key = (db["hostname"], db["port"], db["database"])

    if db_key not in connections:
        reserved_user = os.getenv("SQLSERVER_RESERVED_USER")
        reserved_password = os.getenv("SQLSERVER_RESERVED_PASSWORD")

        # ä¼˜å…ˆä½¿ç”¨ä¿ç•™è´¦å·
        if reserved_user and reserved_password:
            try:
                connections[db_key] = pymssql.connect(
                    server=db["hostname"],
                    port=db["port"],
                    database=db["database"],
                    user=reserved_user,
                    password=reserved_password,
                    as_dict=True,
                    login_timeout=5
                )
                return connections[db_key]
            except Exception as e:
                logging.warning(f"ä½¿ç”¨ä¿ç•™è´¦å·è¿æ¥å¤±è´¥: {e}ï¼Œå°è¯•ä½¿ç”¨åŸå§‹è´¦å·ã€‚")

        # ä¿ç•™è´¦å·ä¸å­˜åœ¨æˆ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹è´¦å·è¿æ¥
        try:
            connections[db_key] = pymssql.connect(
                server=db["hostname"],
                port=db["port"],
                database=db["database"],
                user=db["user"],
                password=db["password"],
                as_dict=True,
                login_timeout=5  # Connection timeout set to 5 seconds
            )
        except Exception as e:
            logging.error(f"ä½¿ç”¨åŸå§‹è´¦å·è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
            raise e

    return connections[db_key]

def get_postgres_connection(db):
    """è·å–æˆ–åˆ›å»ºæ•°æ®åº“è¿æ¥"""
    db_key = (db["hostname"], db["port"], db["database"])

    if db_key not in connections:
        try:
            connections[db_key] = psycopg2.connect(
                host=db["hostname"],
                port=db["port"],
                dbname=db["database"],
                user=db["user"],
                password=db["password"],
                sslmode="disable"
            )
            connections[db_key].autocommit = True
        except Exception as e:
            logging.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise e
        
    return connections[db_key]

# è¯»å– properties æ–‡ä»¶
def load_properties(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            config = ConfigParser()
            # ä¿æŒé”®åçš„åŸå§‹å¤§å°å†™
            config.optionxform = str  # é»˜è®¤ä¼šè½¬æ¢ä¸ºå°å†™ï¼Œæ”¹ä¸ºä¸è½¬æ¢

            properties = "[DEFAULT]\n" + f.read()
            config.read_string(properties)
            
            return dict(config["DEFAULT"])
    except Exception as e:
        logging.error(f"è¯»å– properties æ–‡ä»¶å¤±è´¥: {e}")
        return {}

# è§£æå ä½ç¬¦ï¼Œæå–æ–‡ä»¶è·¯å¾„å’Œå˜é‡å
def extract_file_and_key(value):
    # å¦‚æœ database.hostname å½¢å¦‚ ${file:/etc/kafka-connect/secrets/connect-credentials.properties:IMX_CTO_HOSTNAME}
    # è¯·å°†å…¶æ‹†è§£ä¸º /etc/kafka-connect/secrets/connect-credentials.properties ä»¥åŠ IMX_CTO_HOSTNAME ä¸¤æ®µ

    match = re.match(r"\${file:(.*?):(.*?)}", value)
    return match.groups() if match else (None, None)

# è§£æå ä½ç¬¦å¹¶æ›¿æ¢
def resolve_placeholders(value):

    file_path, key = extract_file_and_key(value)

    # æ–‡ä»¶ä¸å­˜åœ¨æ—¶ä½¿ç”¨é»˜è®¤é…ç½®æ–‡ä»¶
    if file_path and not os.path.exists(file_path):
        file_path = DEFAULT_PROPERTIES_FILE

    if file_path and key:
        properties = load_properties(file_path)
        return properties.get(key, f"<MISSING: {key}>")  # é»˜è®¤å€¼å¤„ç†

    return value  # ä¸åŒ¹é…åˆ™è¿”å›åŸå€¼

def get_debezium_sqlserver_connectors():
    """è·å– Kafka Connect ä¸Šçš„ Debezium SQL Server è¿æ¥å™¨ä¿¡æ¯"""
    response = requests.get(f"{KAFKA_CONNECT_SERVICE_URL}/connectors?expand=info&expand=status")
    if response.status_code != 200:
        logging.error(
            "è·å– Kafka Connect è¿æ¥å™¨å¤±è´¥: çŠ¶æ€ç =%d, å“åº”å†…å®¹=%s, è¯·æ±‚URL=%s",
            response.status_code,
            response.text,
            response.url
        )
        return []
    
    connectors = response.json()
    db_dict = {}
    
    # è¿‡æ»¤ Debezium SQL Server è¿æ¥å™¨
    for connector, details in connectors.items():
        config = details.get("info").get("config")
        status = details.get("status").get("connector").get("state")
        driver = config.get("connector.class")
        
        # åˆ¤æ–­æ˜¯å¦ä¸º Debezium SQL Server è¿æ¥å™¨
        if driver == "io.debezium.connector.sqlserver.SqlServerConnector" or \
            driver == "io.debezium.connector.postgresql.PostgresConnector":
            
            config = {k: resolve_placeholders(v) for k, v in config.items()}
            # fix: å…¼å®¹ Debezium 2.x ç‰ˆæœ¬çš„é…ç½®
            dbname = config.get("database.dbname") if config.get("database.dbname") else config.get("database.names")

            db_key = (dbname, config.get("database.hostname"), config.get("database.port", 1433))
            if db_key not in db_dict:
                db_dict[db_key] = {
                    "driver": driver,
                    "database": dbname,
                    "hostname": config.get("database.hostname"),
                    "port": config.get("database.port", 1433),
                    "user": config.get("database.user"),
                    "password": config.get("database.password"),
                    "tables": set(),
                    "topics": set(),
                    "active_topics": set(),
                    "connectors": set()
                }

            # å¦‚æœ Task æ˜¯ PAUSED çŠ¶æ€ï¼Œåˆ™è·³è¿‡
            if status == "PAUSED":
                logging.warning(f"è¿æ¥å™¨ {connector} å½“å‰çŠ¶æ€ä¸º {status}ï¼Œè·³è¿‡")
                continue

            # æ›´æ–°è¡¨é›†åˆ
            db_dict[db_key]["tables"].update(config.get("table.include.list", "").split(","))

            topics = get_kafka_connect_all_topics(config)
            db_dict[db_key]["topics"].update(topics)

            active_topics = get_kafka_connect_active_topics(config)
            db_dict[db_key]["active_topics"].update(active_topics)

            db_dict[db_key]['connectors'].add(connector)

    return list(db_dict.values())

def get_kafka_connect_active_topics(connector_config):
    """
    è·å–æŒ‡å®š Connector å…³è”çš„ Kafka Topic
    """
    connector_name = connector_config.get("name")

    response = requests.get(f"{KAFKA_CONNECT_SERVICE_URL}/connectors/{connector_name}/topics")
    if response.status_code != 200:
        logging.error(
            "è·å– Kafka Connect è¿æ¥å™¨å¤±è´¥: çŠ¶æ€ç =%d, å“åº”å†…å®¹=%s, è¯·æ±‚URL=%s",
            response.status_code,
            response.text,
            response.url
        )
        return []
        
    connector_topics = response.json()
    return [
        topic for topic in connector_topics.get(connector_name, {}).get('topics', [])
        if '.dbo.' in topic
    ]

# è·å– Active Connect ä¾èµ–çš„æ‰€æœ‰ Topic
def get_kafka_connect_all_topics(connector_config):
    topics = set()

    # 1. topic.prefix æœ¬èº«å¯¹åº”ä¸€ä¸ª topicï¼ˆDebezium ä¼šè®°å½• schema å˜æ›´ï¼‰
    topic_prefix = connector_config.get("topic.prefix")
    if topic_prefix:
        topics.add(topic_prefix)

    # 2. schema.history.internal.kafka.topic
    schema_topic = connector_config.get("schema.history.internal.kafka.topic")
    if schema_topic:
        topics.add(schema_topic)

    # 3. è½¬æ¢åçš„æ•°æ® topicsï¼ˆæ ¹æ® table.include.list å’Œ topic è·¯ç”±è§„åˆ™ï¼‰
    table_include_list = connector_config.get("table.include.list", "")
    tables = [t.strip() for t in table_include_list.split(",") if t.strip()]

    # é»˜è®¤é€»è¾‘ä¸º topic.prefix + "." + table_nameï¼ˆå¦‚æœæ²¡æœ‰ transformï¼‰
    transformed = connector_config.get("transforms", "") == "Reroute"
    if transformed:
        regex = connector_config.get("transforms.Reroute.topic.regex")
        replacement = connector_config.get("transforms.Reroute.topic.replacement")
        # Java -> Python regex replacement compatibility
        replacement_py = replacement.replace("$1", r"\1").replace("$2", r"\2")
        dbname = connector_config.get("database.names")
        if regex and replacement_py:
            # æ¨¡æ‹Ÿ Reroute çš„æ­£åˆ™æ›¿æ¢é€»è¾‘
            for table in tables:
                raw_topic = f"{topic_prefix}.{dbname}.{table}"
                transformed_topic = re.sub(regex, replacement_py, raw_topic)
                topics.add(transformed_topic)
    else:
        logging.warning(f"æœªå¯ç”¨ transformï¼Œå°†ä½¿ç”¨é»˜è®¤æ ¼å¼æ‹¼æ¥ Topicï¼Œè¯·æ£€æŸ¥é…ç½®")
        
        # å¦‚æœæ²¡æœ‰å¯ç”¨ transformï¼ŒæŒ‰é»˜è®¤æ ¼å¼æ‹¼æ¥
        for table in tables:
            topics.add(f"{topic_prefix}.{table}")

    return topics

def check_sqlserver_cdc(db):
    """æ£€æŸ¥ SQL Server æ˜¯å¦å¯ç”¨ CDCï¼Œå¹¶æ¯”å¯¹è¡¨"""
    try:
        conn = get_sqlserver_connection(db)
        cursor = conn.cursor()

        cursor.execute("SELECT is_cdc_enabled FROM sys.databases WHERE name = %s", (db["database"],))
        is_cdc_enabled = cursor.fetchone()
        if not is_cdc_enabled or is_cdc_enabled["is_cdc_enabled"] == 0:
            logging.warning(f"æ•°æ®åº“ {db['database']} æœªå¯ç”¨ CDCï¼Œè¯·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¯ç”¨:")
            logging.info(f"""
            USE {db['database']};
            EXEC sys.sp_cdc_enable_db;
            """)
        
        cursor.execute("SELECT name FROM sys.tables WHERE is_tracked_by_cdc = 1")
        cdc_tables = {"dbo." + row['name'] for row in cursor.fetchall()}
        synced_tables = set(db["tables"])

        need_enable_table_cdc = synced_tables - cdc_tables
        need_disable_table_cdc = cdc_tables - synced_tables

        return True, need_enable_table_cdc, need_disable_table_cdc
    except Exception as e:
        logging.error(f"æ£€æŸ¥ SQL Server CDC å¤±è´¥: {e}")
        return False, set(), set()

def check_sqlserver_cdc_agent_job(db):
    """æ£€æŸ¥ SQL Server CDC Agent Job æ˜¯å¦è¿è¡Œ"""
    conn = get_sqlserver_connection(db)
    cursor = conn.cursor()

    try:
        cursor.execute("EXEC msdb.dbo.sp_help_jobactivity @job_name = %s", (f'cdc.{db["database"]}_capture',))
        job_status = cursor.fetchone()
        if job_status and job_status['start_execution_date'] is not None and job_status['stop_execution_date'] is None:
            logging.info(f"æ•°æ®åº“ {db['database']} çš„ CDC Agent Job {db['database']}_capture æ­£å¸¸è¿è¡Œ")
            return True

        logging.error(f"CDC or LogReader Agent Job for {db['database']} æœªè¿è¡Œï¼Œå°è¯•é‡å¯")
        return False
    except Exception as e:
        # The specified @job_name ('cdc.FISAMS_capture') does not exist.DB-Lib error message 20018, severity 16:
        # General SQL Server error: Check messages from the SQL Server

        # The capture job cannot be used by Change Data Capture to extract changes from the log when transactional replication is also enabled on the same database. 
        # When Change Data Capture and transactional replication are both enabled on a database, use the logreader agent to extract the log changes.

        cursor.execute("SELECT CAST(DATABASEPROPERTYEX(%s, 'IsPublished') AS INT) AS IsPublished", (db["database"]))
        repl_status = cursor.fetchone()
        if repl_status and repl_status['IsPublished'] == 1:
            logging.info(f"æ•°æ®åº“ {db['database']} å¯ç”¨äº† Replicationï¼Œè·³è¿‡æ£€æŸ¥ CDC Agent Job")
            logging.warning(f"æ•°æ®åº“ {db['database']} å¯ç”¨äº† Replicationï¼Œä½†è¦æ³¨æ„ LogReader Agent æ˜¯å®šæ—¶è¿è¡Œï¼Œæ—¶å»¶è¾ƒ CDC æœ‰å·®å¼‚")
            return True

        logging.error(f"CDC or LogReader Agent Job for {db['database']} å‡æœªè¿è¡Œ: {e}")
        return False

def restart_sqlserver_cdc_agent_job(db):
    """é‡å¯ CDC Agent Job"""
    try:
        conn = get_sqlserver_connection(db)
        cursor = conn.cursor()

        cursor.execute("EXEC msdb.dbo.sp_start_job @job_name = %s", (f'cdc.{db["database"]}_capture'))
        logging.info(f"é‡å¯ CDC Agent Job {db['database']}_capture æˆåŠŸ")
    except Exception as e:
        logging.error(f"é‡å¯ CDC Agent Job {db['database']}_capture å¤±è´¥: {e}")

def is_sqlserver_alwayson(db):
    """æ£€æŸ¥æ˜¯å¦ä¸º AlwaysOn é›†ç¾¤"""
    try:
        conn = get_sqlserver_connection(db)
        cursor = conn.cursor()

        cursor.execute("SELECT COUNT(1) AS is_alwayson FROM sys.availability_groups")
        result = cursor.fetchone()
        return result["is_alwayson"] > 0
    except Exception as e:
        logging.error(f"æ£€æŸ¥ AlwaysOn é›†ç¾¤çŠ¶æ€å¤±è´¥: {e}")
        return False

def is_sqlserver_alwayson_primary(db):
    """æ£€æŸ¥å½“å‰å®ä¾‹æ˜¯å¦ä¸º AlwaysOn é›†ç¾¤çš„ä¸»èŠ‚ç‚¹"""
    try:
        conn = get_sqlserver_connection(db)
        cursor = conn.cursor(as_dict=True)

        # ä½¿ç”¨ sys.fn_hadr_is_primary_replica åˆ¤æ–­å½“å‰å®ä¾‹æ˜¯å¦ä¸ºä¸»èŠ‚ç‚¹
        # https://learn.microsoft.com/en-us/sql/relational-databases/system-functions/sys-fn-hadr-is-primary-replica-transact-sql?view=sql-server-ver16
        cursor.execute("""
            SELECT sys.fn_hadr_is_primary_replica(%s) AS is_primary
        """, (db["database"],))
        result = cursor.fetchone()

        if result and (result['is_primary'] == 1 or result['is_primary'] is None):
            logging.info(f"æ•°æ®åº“ {db['database']} æ‰€åœ¨ AlwaysOn é›†ç¾¤èŠ‚ç‚¹ {db['hostname']} æ˜¯ä¸»èŠ‚ç‚¹")
            return True
        else: # is_primary ä¸º 0 åˆ™ä¸æ˜¯ä¸»èŠ‚ç‚¹
            logging.info(f"æ•°æ®åº“ {db['database']} æ‰€åœ¨ AlwaysOn é›†ç¾¤èŠ‚ç‚¹ {db['hostname']} æ˜¯è¾…åŠ©èŠ‚ç‚¹")
            return False
    except Exception as e:
        logging.error(f"æ£€æŸ¥ AlwaysOn ä¸»èŠ‚ç‚¹çŠ¶æ€å¤±è´¥: {e}")
        return False

def get_sqlserver_alwayson_primary(db):
    """è·å– AlwaysOn é›†ç¾¤çš„ä¸»èŠ‚ç‚¹åœ°å€å¹¶æ›´æ–°æ•°æ®åº“é…ç½®"""
    try:
        conn = get_sqlserver_connection(db)
        cursor = conn.cursor()
        
        # 
        cursor.execute("""
            SELECT primary_replica 
            FROM sys.dm_hadr_availability_group_states;
        """)
        result = cursor.fetchone()
        if result:
            logging.info(f"æ•°æ®åº“ {db['database']} æ‰€åœ¨ AlwaysOn é›†ç¾¤çš„ä¸»èŠ‚ç‚¹æ˜¯ {result['primary_replica']}")
            # ä¿®æ”¹æ•°æ®åº“è¿æ¥ä¿¡æ¯æŒ‡å‘ä¸»èŠ‚ç‚¹
            db["hostname"] = result["primary_replica"]
        return db
    except Exception as e:
        logging.error(f"è·å– AlwaysOn ä¸»èŠ‚ç‚¹åœ°å€å¤±è´¥: {e}")
        return db

def check_sqlserver_alwayson_status(db):
    """æ£€æŸ¥ SQL Server AlwaysOn çŠ¶æ€"""
    try:
        conn = get_sqlserver_connection(db)
        cursor = conn.cursor(as_dict=True)

        # https://learn.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-hadr-database-replica-states-transact-sql?view=azuresqldb-current
        cursor.execute("""
            SELECT synchronization_state_desc
            FROM sys.dm_hadr_database_replica_states
            WHERE database_id = DB_ID(%s) 
                AND is_primary_replica = 0
        """, (db["database"],))
        status = cursor.fetchone()
        if status is None:
            logging.info(f"æ•°æ®åº“ {db['database']} ä¸å±äº AlwaysOn é«˜å¯ç”¨ç»„")
            return True

        if status and status['synchronization_state_desc'] != "SYNCHRONIZING":
            logging.error(f"æ•°æ®åº“ {db['database']} å±äº AlwaysOn é«˜å¯ç”¨ç»„ï¼Œä½†åŒæ­¥çŠ¶æ€å¼‚å¸¸: {status['synchronization_state_desc']}")
            return False
        
        logging.info(f"æ•°æ®åº“ {db['database']} å±äº AlwaysOn é«˜å¯ç”¨ç»„ï¼Œä¸”åŒæ­¥çŠ¶æ€æ­£å¸¸")
        return True
    except Exception as e:
        logging.error(f"æ£€æŸ¥ AlwaysOn çŠ¶æ€å¤±è´¥: {e}")
        return False

def apply_sqlserver_cdc_changes(db, enable_tables, disable_tables, dry_run=True):
    """æ ¹æ®è¡¨åé›†åˆå¯ç”¨æˆ–ç¦ç”¨ SQL Server çš„ CDC"""
    try:
        conn = get_sqlserver_connection(db)
        cursor = conn.cursor()

        # å¯ç”¨ CDC
        for table in enable_tables:
            logging.warning(f"å¾…å¯ç”¨è¡¨ {db['hostname']}.{db['database']}.{table} çš„ CDC")

            table_name = table.split(".")[-1]
            sql = f"""
            USE {db['database']};
            EXEC sys.sp_cdc_enable_table
                @source_schema = N'dbo',
                @source_name   = N'{table_name}',
                @role_name     = NULL,
                @supports_net_changes = 1;
            """
            logging.info(sql)
            if not dry_run:
                try:
                    cursor.execute(sql)
                    conn.commit()
                    logging.info(f"å·²å¯ç”¨è¡¨ {table} çš„ CDC")
                except Exception as e:
                    logging.error(f"å¯ç”¨è¡¨ {table} çš„ CDC å¤±è´¥: {e}")

        # ç¦ç”¨ CDC
        for table in disable_tables:
            logging.warning(f"å¾…å…³é—­è¡¨ {db['hostname']}.{db['database']}.{table} çš„ CDC")

            table_name = table.split(".")[-1]
            sql = f"""
            USE {db['database']};
            EXEC sys.sp_cdc_disable_table 
                @source_schema = N'dbo', 
                @source_name = N'{table_name}', 
                @capture_instance = N'dbo_{table_name}';
            """
            logging.info(sql)
            if not dry_run:
                try:
                    cursor.execute(sql)
                    conn.commit()
                    logging.info(f"å·²ç¦ç”¨è¡¨ {table} çš„ CDC")
                except Exception as e:
                    logging.error(f"ç¦ç”¨è¡¨ {table} çš„ CDC å¤±è´¥: {e}")

    except Exception as e:
        logging.error(f"å¯ç”¨æˆ–ç¦ç”¨ SQL Server çš„ CDC å¤±è´¥: {e}")

def check_and_repair_sqlserver_datasource(dbs):

    for db in dbs:
        logging.info("-------------------------------------------------")
        logging.info(f"æ£€æŸ¥æ•°æ®åº“: {db['database']} ({db['hostname']})")

        success, need_enable_table_cdc, need_disable_table_cdc = check_sqlserver_cdc(db)
        if success is False:
            continue

        if is_sqlserver_alwayson(db):
            # æ£€æŸ¥ AlwaysOn ä¸»ä»åŒæ­¥çŠ¶æ€
            if check_sqlserver_alwayson_status(db) is False:
                # TODO:æ¢å¤ä¸»ä»åŒæ­¥çŠ¶æ€
                continue

            # å¦‚æœå½“å‰èŠ‚ç‚¹æ˜¯ SQL Server AlwaysOn é›†ç¾¤çš„è¾…åŠ©è§’è‰²ï¼Œéœ€åˆ‡æ¢åˆ°ä¸»è¦è§’è‰²ï¼Œ
            # å¦åˆ™æ— æ³•æŸ¥è¯¢ Agent è¿è¡ŒçŠ¶æ€
            if not is_sqlserver_alwayson_primary(db):
                db = get_sqlserver_alwayson_primary(db)
        
        # æ£€æŸ¥ SQL Server CDC Agent Job æ˜¯å¦è¿è¡Œ
        if check_sqlserver_cdc_agent_job(db) is False:
            restart_sqlserver_cdc_agent_job(db)
            # continue

        # è‡ªåŠ¨å¼€å¯å…³é—­ CDCï¼Œé»˜è®¤åªæ‰“å°å¾…æ‰§è¡Œçš„æ“ä½œï¼Œä½†å¦‚æœ DRY RUN å…³é—­ï¼Œåˆ™çœŸæ­£æ‰§è¡Œ
        apply_sqlserver_cdc_changes(db, need_enable_table_cdc, need_disable_table_cdc, dry_run=True)


def get_kafka_connect_tasks_status(connector_name):
    """
    è·å– Kafka Connect Job çš„ä»»åŠ¡çŠ¶æ€ã€‚
    """
    try:
        response = requests.get(f"{KAFKA_CONNECT_SERVICE_URL}/connectors/{connector_name}/status")

        if response.status_code != 200:
            logging.error(f"Failed to fetch status for {connector_name}: {response.text}")
            return None

        return response.json()
    except requests.RequestException as e:
        logging.error(f"Failed to connect to Kafka Connect API: {e}")
        return None

def restart_kafka_connect_tasks(connector_name, task_id):
    """
    é‡å¯æŒ‡å®š Kafka Connect ä»»åŠ¡ã€‚
    """
    restart_response = requests.post(f"{KAFKA_CONNECT_SERVICE_URL}/connectors/{connector_name}/tasks/{task_id}/restart")

    if restart_response.status_code in [200, 204]:
        logging.info(f"Successfully restarted task {task_id} of connector {connector_name}.")
        return True
    else:
        logging.error(f"Failed to restart task {task_id} of connector {connector_name}. Response: {restart_response.text}")
        return False

def check_kafka_connect_failed_tasks(trace):
    """
    æ ¹æ®ä»»åŠ¡å¼‚å¸¸ä¿¡æ¯ (trace) åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡å¯ä»»åŠ¡ã€‚
    ä¾‹å¦‚ï¼ŒæŸäº›è‡´å‘½é”™è¯¯ï¼ˆå¦‚é…ç½®é”™è¯¯ï¼‰ä¸é€‚åˆé‡å¯ï¼Œè€Œä¸´æ—¶ç½‘ç»œé”™è¯¯å¯ä»¥å°è¯•é‡å¯ã€‚
    """
    if not trace:
        return True  # æ²¡æœ‰é”™è¯¯ä¿¡æ¯ï¼Œé»˜è®¤å…è®¸é‡å¯

    # å®šä¹‰ä¸å¯é‡å¯çš„é”™è¯¯å…³é”®å­—
    non_restartable_errors = [
        "configured with 'delete.enabled=false' and 'pk.mode=none' and therefore requires records with a non-null Struct value and non-null Struct schema", # ä¸»é”®ä¸ºç©º -- æºå¤´åŠ ä¸»é”®
        "You will need to rewrite or cast the expression", # å­—æ®µç±»å‹é—®é¢˜ -- æœ€å¥½æºå¤´ä¿®æ­£
        "io.confluent.connect.jdbc.sink.TableAlterOrCreateException", # ä¸Šæ¸¸æ–°å¢å­—æ®µï¼Œä¸‹æ¸¸æœªåŒæ­¥å¢åŠ  -- ä¸‹æ¸¸è·Ÿè¿›
    ]
    
    for error in non_restartable_errors:
        if error in trace:
            logging.warning(f"Skipping restart due to critical error: {error}")
            return False

def check_and_restart_kafka_connect_failed_tasks():
    # è·å–æ‰€æœ‰ Connector çŠ¶æ€ä¿¡æ¯
    response = requests.get(f"{KAFKA_CONNECT_SERVICE_URL}/connectors?expand=status")
    
    if response.status_code != 200:
        logging.error(
            "è·å– Kafka Connect è¿æ¥å™¨å¤±è´¥: çŠ¶æ€ç =%d, å“åº”å†…å®¹=%s, è¯·æ±‚URL=%s",
            response.status_code,
            response.text,
            response.url
        )
        return False
    
    connectors_status = response.json()
    
    failed_tasks = []
    for connector_name, details in connectors_status.items():
        tasks = details.get("status").get("tasks")
        for task in tasks:
            if task.get("state") == "FAILED":
                failed_tasks.append((connector_name, task.get("id"), task.get("trace")))
    
    # é‡å¯å¤±è´¥çš„ä»»åŠ¡
    if not failed_tasks:
        logging.info("No failed connectors found.")
        return True
    
    for connector_name, task_id, trace in failed_tasks:

        if not check_kafka_connect_failed_tasks(trace):
            logging.info(f"Skipping restart for {connector_name} - Task {task_id} due to non-restartable error.")
            continue

        restart_kafka_connect_tasks(connector_name, task_id)
    
    return True

def extract_table_name_from_topic(topic):
    """
    ä» Kafka Topic ä¸­æå–è¡¨å
    ä¾‹å¦‚ï¼š
    - input: "dbserver.dbo.users" -> output: "users"
    - input: "dbserver.dbo.orders" -> output: "orders"
    """
    parts = topic.split(".")
    if len(parts) > 2:
        return parts[-1]
    return topic

def get_sqlserver_ct_time(db, table_name):
    """
    æŸ¥è¯¢ SQL Server CT è¡¨çš„æœ€æ–°è®°å½•æ—¶é—´ï¼Œå¹¶åŠ¨æ€è½¬æ¢ä¸º UTC Unix æ—¶é—´æˆ³ (æ¯«ç§’çº§)
    """
    try:
        conn = get_sqlserver_connection(db)  # è·å–æ•°æ®åº“è¿æ¥
        cursor = conn.cursor()

        # è·å– SQL Server å½“å‰æ—¶åŒºåç§»é‡
        cursor.execute("SELECT SYSDATETIMEOFFSET() as server_local_time")
        server_local_time = cursor.fetchone()['server_local_time']  # è¿”å›å¸¦æ—¶åŒºçš„ datetime å¯¹è±¡
        
        # è§£æ SQL Server çš„æ—¶åŒºåç§»é‡
        utcoffset = server_local_time.utcoffset()  # è·å–æ—¶åŒºåç§»é‡ (timedelta)

        # æŸ¥è¯¢æœ€æ–°çš„ CDC è®°å½•æ—¶é—´
        cursor.execute(f"""
            SELECT TOP 1 
                sys.fn_cdc_map_lsn_to_time(__$start_lsn) AS latest_lsn_time,
                *
            FROM [cdc].[dbo_{table_name}_CT]
            ORDER BY __$start_lsn DESC;
        """)

        latest_record = cursor.fetchone()
        
        if latest_record:
            latest_lsn_time = latest_record['latest_lsn_time']  # SQL Server è¿”å›çš„æœ¬åœ°æ—¶é—´ (without timezone)
            
            # æ—¶é—´é™„åŠ ä¸Šæ—¶åŒºä¿¡æ¯ 
            latest_lsn_time = latest_lsn_time.replace(tzinfo=timezone(utcoffset))  
            
            # è½¬æ¢ä¸º UTC
            lsn_utc_time = latest_lsn_time.astimezone(timezone.utc)
            
            # è½¬æ¢ä¸º Unix æ—¶é—´æˆ³ (æ¯«ç§’çº§)
            return int(lsn_utc_time.timestamp() * 1000)  

        return None
    except Exception as e:
        logging.error(f"æ£€æŸ¥ SQL Server CT å¤±è´¥: {e}")
        return None
# https://github.com/confluentinc/confluent-kafka-python/blob/master/examples/get_watermark_offsets.py
def get_kafka_ct_time(topic):
    """
    è·å– Kafka Topic å„åˆ†åŒºæœ€æ–°çš„ä¸€æ¡æ¶ˆæ¯ï¼Œå¹¶è¿”å›æœ€æ–°çš„æ—¶é—´æˆ³
    """
    try:
        consumer = Consumer({
            'bootstrap.servers': KAFKA_CONNECT_BOOTSTRAP_SERVERS,
            'group.id': 'monitoring-consumer',
            'auto.offset.reset': 'latest',
            'enable.auto.commit': False
        })

        # Get the topic's partitions
        metadata = consumer.list_topics(topic)
        if metadata.topics[topic].error is not None:
            raise KafkaException(metadata.topics[topic].error)

        # Construct TopicPartition list of partitions to query
        partitions = [TopicPartition(topic, p) for p in metadata.topics[topic].partitions.keys()]

        latest_timestamp = None

        for tp in partitions:
            # è·å–åˆ†åŒºçš„ offset
            low_offset, high_offset = consumer.get_watermark_offsets(tp, cached=False)
            if high_offset == 0 or high_offset == low_offset:
                continue  # è¯¥åˆ†åŒºæ²¡æœ‰æ•°æ®ï¼Œè·³è¿‡

            # é‡æ–°åˆ›å»º TopicPartition å¯¹è±¡ï¼Œè®¾ç½® offset
            tp_with_offset = TopicPartition(tp.topic, tp.partition, high_offset - 1)
            
            # fix: Error: KafkaError{code=_STATE,val=-172,str="Failed to seek to offset 7554142: Local: Erroneous state"}
            # consumer.seek(tp_with_offset)
            consumer.assign([tp_with_offset])

            msg = consumer.poll(timeout=5.0)  # è·å–æœ€æ–°æ¶ˆæ¯
            if msg and not msg.error():
                msg_ts = msg.timestamp()[1]
                if latest_timestamp is None or msg_ts > latest_timestamp:
                    latest_timestamp = msg_ts

        consumer.close()

        return latest_timestamp
    except Exception as e:
        logging.error(f"Error occurred while checking kafka_ct_time of topic {topic}: {e}")
        return None

def check_sqlserver2kafka_sync(ds, max_retries=3):
    for db in ds:
        logging.info("-------------------------------------------------")
        logging.info(f"æ£€æŸ¥æ•°æ®åº“: {db['database']} ({db['hostname']})")

        # å¤„ç† active_topicsï¼Œéšæœºé€‰æ‹©ä¸€ä¸ª Topic
        active_topics = db.get("active_topics", [])
        if not active_topics:
            logging.warning(f"[WARN] æ•°æ®åº“ {db['database']} æ²¡æœ‰ active_topics")
            continue

        attempts = 0
        while attempts < max_retries and active_topics:
            random_topic = random.choice(list(active_topics))
            table_name = extract_table_name_from_topic(random_topic)
            logging.info(f"ğŸ² éšæœºé€‰æ‹© Topic: {random_topic} -> è¡¨å: {table_name}")

            # æŸ¥è¯¢ CT è¡¨æœ€æ–°æ—¶é—´
            ct_time = get_sqlserver_ct_time(db, table_name)
            if not ct_time:
                logging.warning(f"[WARN] æŸ¥è¯¢ CT è¡¨ {table_name} æ²¡æœ‰æ•°æ®ï¼Œç§»é™¤ Topic [{random_topic}]ï¼Œé‡è¯• {attempts+1}/{max_retries}")
                active_topics.remove(random_topic)  # ä»åˆ—è¡¨ä¸­ç§»é™¤æ— æ•°æ®çš„ Topic
                attempts += 1
                continue  # é‡æ–°é€‰æ‹©ä¸€ä¸ª Topic å†è¯•
            break  # æˆåŠŸè·å–åˆ° CT æ—¶é—´ï¼Œåˆ™é€€å‡ºé‡è¯•å¾ªç¯

        if not ct_time:
            logging.error(f"[ERROR] æ•°æ®åº“ {db['database']} é‡Œæ‰€æœ‰ Topic éƒ½æ²¡æœ‰ CT æ•°æ®ï¼Œè·³è¿‡")
            continue

        # æŸ¥è¯¢ Kafka æœ€æ–°æ¶ˆè´¹æ—¶é—´
        kafka_time = get_kafka_ct_time(random_topic)
        if not kafka_time:
            logging.warning(f"[WARN] æŸ¥è¯¢ Topic {random_topic} æ²¡æœ‰æ•°æ®")
            continue

        # è®¡ç®—æ—¶é—´å·®
        time_diff = (kafka_time - ct_time) / 1000
        logging.info(f"ğŸ“Š æ—¶é—´å·®: {time_diff:.3f} ç§’")

        if time_diff > 60:
            logging.warning(f"[WARN] æ—¶é—´å·®è¶…è¿‡é˜ˆå€¼: {time_diff:.3f} ç§’")
            # TODO:å°è¯•é‡å¯ Debezium Connect

        else:
            logging.info(f"âœ… æ—¶é—´å·®åœ¨é˜ˆå€¼èŒƒå›´å†…: {time_diff:.3f} ç§’")

    return True

# æ£€æµ‹ CDC Topic æ˜¯å¦æœ‰ Lag
def check_kafka2postgres_sync():
    high_lag_groups = get_kafka_lag_consumer_groups_v2()

    if not high_lag_groups:
        logging.info("No high lag consumer groups found.")
        return True

    # ç”Ÿæˆ Kafka Connect Job åç§°å¹¶é‡å¯
    for group_id in high_lag_groups:
        # å»æ‰ `connect-` å‰ç¼€ï¼Œå°±æ˜¯ JDBC Connect Job åç§°
        connector_name = group_id.replace("connect-", "", 1)
        
        tasks_status = get_kafka_connect_tasks_status(connector_name)
        if tasks_status is None:
            logging.error(f"Skipping restart: Failed to retrieve status for {connector_name}.")
            continue

        tasks = tasks_status.get("tasks", [])
        restart_needed = False

        for task in tasks:
            task_id = task.get("id")
            task_state = task.get("state")
            trace = task.get("trace", "")

            if task_state == "FAILED":
                if check_kafka_connect_failed_tasks(trace):
                    logging.warning(f"Task {task_id} of {connector_name} is FAILED. Restarting ...")
                    restart_needed = True
                else:
                    logging.warning(f"Skipping restart for {connector_name} - Task {task_id} due to non-restartable error.")
            
            elif task_state in ["RUNNING", "UNASSIGNED"]:
                logging.info(f"Task {task_id} of {connector_name} is {task_state}. Restarting due to high lag ...")
                restart_needed = True

            # æ‰§è¡Œé‡å¯
            if restart_needed:
                restart_kafka_connect_tasks(connector_name, task_id)

    return True

def get_kafka_lag_consumer_groups():
    """
    è·å– Kafka æ‰€æœ‰ consumer group çš„ lag ä¿¡æ¯ï¼ˆä¼˜åŒ–ä¸ºæ‰¹é‡æŸ¥è¯¢ï¼‰
    """
    try:
        admin_client = AdminClient({
            'bootstrap.servers': KAFKA_CONNECT_BOOTSTRAP_SERVERS
        })
        
        consumer = Consumer({
            'bootstrap.servers': KAFKA_CONNECT_BOOTSTRAP_SERVERS,
            'group.id': 'monitoring-consumer',
            'auto.offset.reset': 'latest',
            'enable.auto.commit': False
        })

        # è·å–æ‰€æœ‰ consumer group
        future_groups = admin_client.list_consumer_groups(request_timeout=10)
        groups = future_groups.result()
        
        consumer_group_ids = [ConsumerGroupTopicPartitions(g.group_id) for g in groups.valid 
                              if g.group_id.startswith("connect-")]
        if not consumer_group_ids:
            logging.warning("No consumer groups match the filter.")
            return []
        
        logging.info(f"Fetching offsets for {len(consumer_group_ids)} consumer groups...")

        high_lag_groups = []

        # TODO:ç”±äº lib åº“ä¸æ”¯æŒæ‰¹é‡æŸ¥è¯¢çš„æ“ä½œï¼Œæ‰€ä»¥åªèƒ½å¾ªç¯å¤„ç†
        for consumer_group_id in consumer_group_ids:

            # æ‰¹é‡è¯·æ±‚ consumer group çš„ offsets
            future_offsets = admin_client.list_consumer_group_offsets([consumer_group_id])

            group_id = consumer_group_id.group_id
            offsets = future_offsets[group_id].result()
            
            logging.debug(f"Consumer Group: {group_id}")
            summedLag = 0
            for tp in offsets.topic_partitions:
                # è·å– partition æœ€æ–° offsetï¼ˆhigh watermarkï¼‰
                high_watermark = consumer.get_watermark_offsets(tp)[1]  # (low, high)
                lag = high_watermark - tp.offset  # è®¡ç®— lag
                summedLag += lag
                logging.debug(f"  Topic: {tp.topic}, Partition: {tp.partition}, Lag: {lag}")

            # è¿‡æ»¤å‡º lag > 100
            if summedLag > 200:
                logging.info(f"High lag consumer group: {group_id}, Summed Lag: {summedLag}")
                high_lag_groups.append(group_id)
            
        return high_lag_groups
    except Exception as e:
        logging.error(f"Error fetching consumer groups: {e}")
        return []

# é€šè¿‡ RedPanda API è·å– Consumer Groups
def get_kafka_lag_consumer_groups_v2():
    response = requests.get(f"{REDPANDA_API}/api/consumer-groups")
    if response.status_code != 200:
        logging.error("Failed to fetch consumer groups")
        return []

    data = response.json()

    # è¿‡æ»¤å‡º lag > 100
    high_lag_groups = []
    for group in data.get("consumerGroups", []):
        if group["groupId"].startswith("connect-") is False:
            continue

        for topic in group.get("topicOffsets", []):
            if topic["summedLag"] > 200:
                logging.info(f"High lag consumer group: {group['groupId']}, Summed Lag: {topic['summedLag']}")
                high_lag_groups.append(group["groupId"])
                break  # åªè¦æœ‰ä¸€ä¸ª topic lag è¶…è¿‡ 1000ï¼Œå°±æ ‡è®°è¯¥ group éœ€è¦é‡å¯
    
    return high_lag_groups


def check_sqlserver2postgres_sync(ds, method="KAFKA"):

    if method.upper() == "KAFKA":
        check_sqlserver2kafka_sync(ds)

        check_and_restart_kafka_connect_failed_tasks()

        check_kafka2postgres_sync()
    elif method.upper() == "SEATUNNEL":
        check_sqlserver2postgres_sync_v2(ds)
    else:
        logging.error("ä¸æ”¯æŒçš„ method")
        return

def check_sqlserver2postgres_sync_v2(ds):
    # TODO:å¾…å®ç°ï¼Œéœ€è¦ç»“åˆè¡€ç¼˜ï¼Œè·å–æºç«¯ + ç›®æ ‡ç«¯çš„è¿æ¥ä¿¡æ¯
    return 


def check_postgres_replication_slot(db):
    try:
        conn = get_postgres_connection(db)
        cursor = conn.cursor()
        cursor.execute("SELECT active FROM pg_replication_slots WHERE slot_name = %s", (db["replication_slot"],))
        result = cursor.fetchone()
        cursor.close()
        if result and result[0] is True:
            logging.info(f"æ•°æ®åº“ {db['database']} replication slot {db['replication_slot']} è¿è¡Œæ­£å¸¸")
            return True
        else:
            logging.error(f"æ•°æ®åº“ {db['database']} replication slot {db['replication_slot']} è¿è¡Œå¼‚å¸¸")
            return False
    except Exception as e:
        logging.error(f"æ£€æŸ¥æ•°æ®åº“ {db['database']} replication slot {db['replication_slot']} è¿è¡ŒçŠ¶æ€å¤±è´¥: {e}")
        return False

def should_drop_postgres_replication_slot(trace):
    non_recoverable_keywords = [
        "ERROR: unexpected duplicate for tablespace 0, relfilenode", # æ ¹å› æ˜¯ä¸ªè°œï¼Œä½†åªè¦é‡åˆ°å°±å¾—é‡å»º
    ]
    return any(keyword in trace for keyword in non_recoverable_keywords)

def drop_postgres_replication_slot(db):
    try:
        conn = get_postgres_connection(db)
        cursor = conn.cursor()
        cursor.execute("SELECT pg_drop_replication_slot(%s)", (db['replication_slot'],))
        logging.warning(f"å·²åˆ é™¤ replication slot: {db['replication_slot']}")
        cursor.close()
    except Exception as e:
        logging.error(f"åˆ é™¤ replication slot {db['replication_slot']} å¤±è´¥: {e}")

def check_and_repair_postgres_datasource(dbs):

    for db in dbs:
        # å¦‚æœ replication slot æœªè®¾ç½®ï¼Œåˆ™é»˜è®¤ä¸º 'debezium'
        if not db.get('replication_slot'):
            db['replication_slot'] = 'debezium'

        logging.info("-------------------------------------------------")
        logging.info(f"æ£€æŸ¥æ•°æ®åº“: {db['database']} ({db['hostname']})")
        
        if check_postgres_replication_slot(db):
            continue

        logging.warning(f"å°è¯•æ¢å¤ connector: {db['connectors']}")

        for connector in db['connectors']:
            tasks_status = get_kafka_connect_tasks_status(connector)
            if tasks_status is None:
                continue

            tasks = tasks_status.get("tasks", [])
            restart_needed = False
            recreate_needed = False

            for task in tasks:
                task_id = task.get("id")
                task_state = task.get("state")
                trace = task.get("trace", "")

                if task_state == "FAILED":
                    # å¤±è´¥çš„ä»»åŠ¡è¦ä¹ˆé‡å¯èƒ½æ¢å¤ï¼Œè¦ä¹ˆé‡å¯æ¢å¤ä¸äº†ï¼Œåªèƒ½é‡å»º
                    if should_drop_postgres_replication_slot(trace):
                        logging.warning(f"Skipping restart for {connector} - Task {task_id} due to non-restartable error.")
                        recreate_needed = True
                    else:
                        logging.warning(f"Task {task_id} of {connector} is FAILED. Restarting ...")
                        restart_needed = True
                
                elif task_state in ["RUNNING", "UNASSIGNED"]:
                    # å…ˆé‡å¯ä¸€æŠŠç…ç…
                    logging.info(f"Task {task_id} of {connector} is {task_state}, but replication slot is inactive. Restarting task to recover.")
                    restart_needed = True

                if recreate_needed:
                    drop_postgres_replication_slot(db)

                if restart_needed:
                    restart_kafka_connect_tasks(connector, task_id)

def check_postgres2kafka_sync(ds):
    return

def get_kafka_topics():
    # TODO:Kafka Native API
    return []

def get_kafka_topics_v2():
    try:
        response = requests.get(f"{REDPANDA_API}/api/topics")
        response.raise_for_status()
        data = response.json()
        return {topic['topicName'] for topic in data.get('topics', [])}
    except requests.RequestException as e:
        logging.error(f"è·å– Kafka Topic åˆ—è¡¨å¤±è´¥: {e}")
        return set()

def check_and_repair_kafka_topics(dbs):
    all_topics = get_kafka_topics_v2()
    if not all_topics:
        logging.error("æœªèƒ½è·å– Kafka Topic åˆ—è¡¨ï¼Œç»ˆæ­¢æ£€æŸ¥ã€‚")
        return

    for db in dbs:
        logging.info("-------------------------------------------------")
        logging.info(f"æ£€æŸ¥æ•°æ®åº“: {db['database']} ({db['hostname']})")

        expected_topics = set(db.get('topics', []))               # B
        active_topics = set(db.get('active_topics', []))          # C
        # å‰ç¼€åŒ¹é…çš„åˆ¤æ–­æ¡ä»¶ï¼ˆå¸¦ . çš„ï¼‰
        prefixes = {t.rsplit('.', 1)[0] + '.' for t in expected_topics if '.' in t}
        # ç­‰å€¼åŒ¹é…çš„åˆ¤æ–­æ¡ä»¶ï¼ˆä¸å¸¦ . çš„ï¼‰
        exact_matches = {t for t in expected_topics if '.' not in t}
        # åŒ¹é…é€»è¾‘ï¼š
        matching_topics = {                                      # A
            t for t in all_topics
            if any(t.startswith(prefix) for prefix in prefixes) or t in exact_matches
        }

        # å·®é›†åˆ†æ
        deprecated_topics = matching_topics - expected_topics  # A - B
        missing_topics = expected_topics - matching_topics     # B - A
        inactive_topics = expected_topics - active_topics      # B - C

        # è¾“å‡ºåºŸå¼ƒ Topic
        if deprecated_topics:
            logging.warning(f"å‘ç°åºŸå¼ƒçš„ Topicï¼ˆA - Bï¼‰: {deprecated_topics}")
        else:
            logging.info("æ— åºŸå¼ƒçš„ Topicï¼ˆA - Bï¼‰")

        # è¾“å‡ºç¼ºå¤± Topic
        if missing_topics:
            logging.error(f"ç¼ºå¤±çš„ Topicï¼Œéœ€è¦åˆ›å»ºï¼ˆB - Aï¼‰: {missing_topics}")
        else:
            logging.info("æ— ç¼ºå¤±çš„ Topicï¼ˆB - Aï¼‰")

        # æ´»è·ƒåº¦æ£€æŸ¥
        if expected_topics:
            active_ratio = (len(expected_topics) - len(inactive_topics)) / len(expected_topics)
            logging.info(f"å½“å‰æ´»è·ƒ Topic æ•°é‡: {len(expected_topics) - len(inactive_topics)} / {len(expected_topics)}ï¼ˆæ´»è·ƒåº¦: {active_ratio:.2%}ï¼‰")
        else:
            logging.warning("æœªå®šä¹‰æœŸæœ› Topic é›†åˆï¼ˆBï¼‰ï¼Œæ— æ³•è¯„ä¼°æ´»è·ƒåº¦ã€‚")

    return

def main():
    try:
        ds = get_debezium_sqlserver_connectors()

        sqlds = [d for d in ds if d.get("driver") == "io.debezium.connector.sqlserver.SqlServerConnector"]

        logging.info("-------------------------------------------------")
        logging.info("SQL Server CDC å·¡æ£€å¼€å§‹")

        check_and_repair_sqlserver_datasource(sqlds)

        logging.info("-------------------------------------------------")
        logging.info("SQL Server CDC å·¡æ£€å®Œæˆ")

        logging.info("-------------------------------------------------")
        logging.info("Kafka Topics å·¡æ£€å¼€å§‹")

        check_and_repair_kafka_topics(sqlds)

        logging.info("-------------------------------------------------")
        logging.info("Kafka Topics å·¡æ£€å®Œæˆ")

        logging.info("-------------------------------------------------")
        logging.info("SQL Server2Postgres CDC åŒæ­¥å·¡æ£€å¼€å§‹")

        check_sqlserver2postgres_sync(sqlds, "KAFKA")

        logging.info("-------------------------------------------------")
        logging.info("SQL Server2Postgres CDC åŒæ­¥å·¡æ£€å®Œæˆ")

        pgds = [d for d in ds if d.get("driver") == "io.debezium.connector.postgresql.PostgresConnector"]
        
        logging.info("-------------------------------------------------")
        logging.info("PostgreSQL CDC å·¡æ£€å¼€å§‹")

        check_and_repair_postgres_datasource(pgds)

        logging.info("-------------------------------------------------")
        logging.info("PostgreSQL CDC å·¡æ£€å®Œæˆ")

        check_postgres2kafka_sync(pgds)

    except Exception as e:
        logging.error(f"è¿è¡Œå‡ºé”™: {e}")

if __name__ == "__main__":
    main()